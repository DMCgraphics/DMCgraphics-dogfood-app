# AI Meal Recommendation System

## Overview

Enhanced AI meal recommendation system with hybrid architecture combining rule-based scoring and LLM explanations for transparent, trustworthy recommendations.

## Architecture

### Rule-Based Scoring (`ai-meal-recommendations.ts`)
- 10+ scoring factors (age, activity, body condition, weight goals, health goals, breed, allergens, portions)
- Detailed scoring breakdowns with point contributions
- Missing data detection
- Edge case flagging

### LLM Integration (`llm-service.ts`)
- Anthropic Claude 3.5 Sonnet API
- Request batching for efficiency
- 3-layer caching (session, user, global)
- Rate limiting (10 requests/minute default)
- Automatic fallback to templates on API failure

### Confidence Calculator (`confidence-calculator.ts`)
- Converts numeric scores (0-100) to user-friendly levels
- Four levels: Very High (85+), High (70-84), Moderate (55-69), Needs More Info (<55)
- Color-coded visualization system
- Badge and progress bar styling

### Prompt Templates (`prompt-templates.ts`)
- Structured prompts for consistency
- Warm, conversational personality
- Science-based but not technical
- Template fallbacks for offline operation

## Configuration

### Environment Variables

Add to `.env.local`:

```bash
# Anthropic Claude API
ANTHROPIC_API_KEY=your_key_here

# AI Configuration
NEXT_PUBLIC_ENABLE_AI_LLM=false      # Set to true when ready
AI_ENABLE_BATCHING=true              # Parallel LLM requests
AI_FALLBACK_TO_TEMPLATES=true        # Use templates when LLM fails
AI_RATE_LIMIT_PER_MINUTE=10          # Max LLM requests per minute
AI_CACHE_TTL_DAYS=7                  # Cache lifetime
```

### Getting Started

1. **Get API Key**: Visit https://console.anthropic.com/
2. **Add to `.env.local`**: Replace `your_key_here` with your actual key
3. **Enable LLM**: Set `NEXT_PUBLIC_ENABLE_AI_LLM=true`
4. **Restart dev server**: `npm run dev`

### Cost Management

- **Per session**: ~$0.002-0.005 with 70% cache hit rate
- **Monthly (1000 sessions)**: ~$2.10
- **Optimizations**:
  - 3-layer caching reduces API calls by 70%
  - Request batching minimizes overhead
  - Rate limiting prevents runaway costs
  - Automatic fallback to templates

## Data Structures

### AIRecommendation

```typescript
interface AIRecommendation {
  dogId: string
  dogName: string
  recommendedRecipes: string[]
  reasoning: string
  confidence: number
  nutritionalFocus: string[]

  // Enhanced fields
  confidenceBreakdown?: ConfidenceBreakdown
  llmExplanation?: string
  factorsConsidered: ScoringFactor[]
  alternativeRecommendations?: AlternativeRecommendation[]
  missingData?: string[]
  edgeCases?: string[]
}
```

### ScoringFactor

```typescript
interface ScoringFactor {
  factor: string              // "Weight Loss Formula"
  points: number              // 18
  description: string         // "Lower fat (12%) + higher fiber..."
  impact: 'high' | 'medium' | 'low'
  category: 'age' | 'activity' | 'weight' | 'health' | 'breed' | 'allergens' | 'portions'
}
```

### ConfidenceBreakdown

```typescript
interface ConfidenceBreakdown {
  baseScore: number              // 50
  adjustments: ConfidenceAdjustment[]
  totalScore: number             // 87
  confidenceLevel: string        // "Very High Match"
}
```

## Usage Examples

### Generate Recommendations

```typescript
import { generateAIMealRecommendations } from '@/lib/ai-meal-recommendations'

const recommendations = generateAIMealRecommendations([dogProfile])

console.log(recommendations[0].confidence)           // 87
console.log(recommendations[0].confidenceBreakdown)  // Detailed breakdown
console.log(recommendations[0].factorsConsidered)    // All scoring factors
console.log(recommendations[0].alternativeRecommendations) // Other options
```

### Get LLM Explanation

```typescript
import { getConfidenceExplanation } from '@/lib/ai/llm-service'

const explanation = await getConfidenceExplanation(
  87,
  'Max',
  topFactors,
  missingData
)

// Returns: "I'm very confident in this recommendation for Max! His weight
// loss goal, high activity level, and body condition score all point to
// this low-fat, high-protein formula. âœ¨"
```

### Calculate Confidence Level

```typescript
import { calculateConfidence } from '@/lib/ai/confidence-calculator'

const result = calculateConfidence(87)

console.log(result.level)        // 'very-high'
console.log(result.emoji)        // 'âœ¨'
console.log(result.label)        // 'Very High Match'
console.log(result.bgColor)      // 'bg-emerald-50'
```

## Development Status

### âœ… Phase 1: Foundation (Week 1) - COMPLETE

- [x] Anthropic Claude API integration
- [x] Prompt templates
- [x] Confidence calculator
- [x] Enhanced data structures
- [x] Scoring algorithm with breakdowns
- [x] Environment variables

### ðŸš§ Phase 2: Enhanced UI Components (Week 2) - NEXT

- [ ] Enhanced AIRecommendationCard
- [ ] AIInlineHelper component
- [ ] Confidence visualization
- [ ] Basic LLM explanations
- [ ] Loading states

### ðŸ“‹ Phase 3-6: Coming Soon

- Phase 3: Trust-Building Features
- Phase 4: Engagement Features
- Phase 5: Performance & Polish
- Phase 6: Testing & Launch

## Testing

### Without LLM (Template Fallbacks)

Set `NEXT_PUBLIC_ENABLE_AI_LLM=false` to test with template responses:

```typescript
// LLM disabled - uses template
const explanation = await getConfidenceExplanation(87, 'Max', [], [])
// Returns: "I'm very confident in this recommendation for Max! âœ¨"
```

### With LLM (API Calls)

Set `NEXT_PUBLIC_ENABLE_AI_LLM=true` and provide valid API key:

```typescript
// LLM enabled - makes API call
const explanation = await getConfidenceExplanation(87, 'Max', topFactors, [])
// Returns: Personalized explanation based on actual factors
```

### Cache Testing

```typescript
import { getCacheStats, clearSessionCache } from '@/lib/ai/llm-service'

// Check cache status
const stats = getCacheStats()
console.log(`Cache size: ${stats.size} entries`)

// Clear cache
clearSessionCache()
```

## Monitoring

### Log AI Activity

All AI operations log to console with `[LLM Service]` prefix:

```
[LLM Service] Request completed in 347ms
[LLM Service] Cache hit: abc123
[LLM Service] Rate limit: 7/10 requests used
```

### Track Costs

Monitor token usage in responses:

```typescript
const response = await generateLLMResponse(prompt)
console.log(`Tokens used: ${response.tokensUsed}`)
console.log(`Cached: ${response.cached}`)
```

## Troubleshooting

### "LLM not configured" Error

- Check `ANTHROPIC_API_KEY` is set in `.env.local`
- Verify `NEXT_PUBLIC_ENABLE_AI_LLM=true`
- Restart dev server after env changes

### High Costs

- Check cache hit rate: `getCacheStats()`
- Lower `AI_RATE_LIMIT_PER_MINUTE`
- Verify `AI_ENABLE_BATCHING=true`
- Ensure `AI_FALLBACK_TO_TEMPLATES=true`

### Rate Limit Exceeded

- Increase `AI_RATE_LIMIT_PER_MINUTE` (careful with costs)
- Implement user-level rate limiting
- Add request queue with exponential backoff

## Next Steps

1. **Phase 2**: Build enhanced UI components
2. **Phase 3**: Add transparency features
3. **Phase 4**: Implement engagement features
4. **Phase 5**: Optimize performance
5. **Phase 6**: Launch with A/B testing

See full implementation plan: `/Users/dylancohen/.claude/plans/fizzy-watching-robin.md`
